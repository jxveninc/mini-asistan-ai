# my_smart_assistant/assistant_core.py
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import wikipedia # Wikipedia API kÃ¼tÃ¼phanesi
import time 
from fuzzywuzzy import fuzz, process # <-- BU Ä°KÄ° SATIRI EKLEYÄ°N
import re # Metin temizleme iÃ§in

from knowledge_base import KnowledgeBase
from web_scraper import fetch_and_clean_text # Bu sadece yapÄ±sal olarak duruyor

class SmartAssistant:
    def __init__(self, kb_filename="knowledge_base.json", feedback_callback=None):
        self.kb = KnowledgeBase(kb_filename)
        self.feedback = feedback_callback
        
        try:
            nltk.download('punkt', quiet=True) 
        except Exception as e:
             self._send_feedback(f"NLTK indirme hatasÄ±: {e}. LÃ¼tfen internet baÄŸlantÄ±nÄ±zÄ± kontrol edin.")
             
        # Q&A Ã§ifti Ã¼retim kurallarÄ± (sabit ve Wikipedia uyumlu)
        self.qa_rules_config = {'min_sentence_len': 20} 

    def _send_feedback(self, message):
        """Geri bildirim fonksiyonu tanÄ±mlÄ±ysa mesajÄ± GUI'ye gÃ¶nderir."""
        if self.feedback:
            # GUI main thread'inde Ã§alÄ±ÅŸmasÄ± iÃ§in bu mekanizma kullanÄ±lÄ±r
            self.feedback(message)

    def _clean_text(self, text):
        """
        Metni kÃ¼Ã§Ã¼k harfe Ã§evirir, noktalama ve gereksiz boÅŸluklarÄ± kaldÄ±rÄ±r.
        """
        # Sadece harf, rakam ve boÅŸluklarÄ± tut
        cleaned = re.sub(r'[^\w\s]', '', text).lower()
        # Fazla boÅŸluklarÄ± tek boÅŸluÄŸa dÃ¼ÅŸÃ¼r
        return re.sub(r'\s+', ' ', cleaned).strip()

    def _generate_qa_from_sentence(self, sentence):
        """
        Gelen cÃ¼mleden Soru-Cevap Ã§ifti Ã¼retir. Kontrol mekanizmasÄ± gevÅŸetildi.
        """
        
        if len(sentence) < 20: 
             return None, None
             
        tokens = word_tokenize(sentence)
        
        if len(tokens) > 3:
            # Soru iÃ§in cÃ¼mlenin ilk 4 kelimesini alÄ±yoruz
            limit = min(4, len(tokens) - 1)
            keyword_phrase = ' '.join(tokens[:limit]).lower()
            
            # Soru formatÄ±nÄ± doÄŸrudan anahtar kelimeden tÃ¼ret.
            soru = f"{keyword_phrase} nedir?"
            cevap = sentence.capitalize()
            
            # KONTROL GEVÅETÄ°LÄ°YOR: ArtÄ±k sadece ilk kelimeyi kontrol etmiyoruz,
            # cevap olarak kaydetmeyi deniyoruz. Zaten tek cÃ¼mlelik Ã¶zet olduÄŸu iÃ§in
            # alakasÄ±z olma ihtimali Ã§ok dÃ¼ÅŸÃ¼ktÃ¼r.
            # Anahtar kelimenin ilk token'Ä±nÄ±n cevapta geÃ§mesi yeterli.
            
            first_keyword = keyword_phrase.split()[0].lower()
            
            if len(first_keyword) > 2 and first_keyword in self._clean_text(cevap).lower():
                return soru.capitalize(), cevap
            
        return None, None

    def learn_from_text(self, text, source_name="Wikipedia KaynaÄŸÄ±"):
        """Verilen metinden Soru-Cevap Ã§iftleri Ã¼retir ve hafÄ±zaya kaydeder."""
        if not text:
            return 0 

        try:
            # TÃ¼rkÃ§e metin iÃ§in 'turkish' dilini kullanmayÄ± deneriz
            sentences = sent_tokenize(text, language='turkish')
        except LookupError:
            sentences = sent_tokenize(text, language='english') 
        
        new_knowledge_count = 0
        for sentence in sentences:
            soru, cevap = self._generate_qa_from_sentence(sentence)
            
            if soru and cevap and len(soru) > 10:
                if self.kb.add_knowledge(source_name, soru, cevap):
                    new_knowledge_count += 1
        return new_knowledge_count

    
    def search_and_learn_online(self, query, num_results=1):
        """
        DoÄŸrudan Wikipedia API'sini kullanarak arama yapar ve Ã¶ÄŸrenir.
        """
        self._send_feedback(f"Wikipedia Arama baÅŸlatÄ±ldÄ±: '{query}'")
        total_learned = 0
        
        # YENÄ° KOD: Sorgudan gereksiz kelimeleri (nedir, kimdir vb.) ve noktalama iÅŸaretlerini kaldÄ±r
        cleaned_search_query = query.lower().replace('nedir', '').replace('kimdir', '').replace('?', '').strip()
        
        if not cleaned_search_query: # EÄŸer sadece "nedir?" sorulmuÅŸsa
             cleaned_search_query = query 
             
        try:
            wikipedia.set_lang("tr") 
            
            # Sorguyu temizlenmiÅŸ haliyle gÃ¶nderiyoruz.
            raw_text = wikipedia.summary(cleaned_search_query, sentences=1, auto_suggest=True) 
            
            # Sayfa baÅŸlÄ±ÄŸÄ±nÄ± Ã§ekelim (auto_suggest olduÄŸu iÃ§in doÄŸru baÅŸlÄ±k bulunur)
            page = wikipedia.page(cleaned_search_query, auto_suggest=True)
            page_title = page.title

            self._send_feedback(f"Sayfa Ã¶zeti Ã§ekiliyor: '{page_title}'")
            
            if not raw_text:
                self._send_feedback("Sayfadan iÃ§erik Ã§ekilemedi (boÅŸ metin).")
                return 0
            
            learned_from_wiki = self.learn_from_text(raw_text, f"Wikipedia Ã–zeti: {page_title}")
            
            self._send_feedback(f"-> {page_title} Ã¶zetinden {learned_from_wiki} adet bilgi Ã¶ÄŸrenildi.")
            total_learned += learned_from_wiki

        except wikipedia.exceptions.PageError:
            self._send_feedback("Wikipedia Sayfa HatasÄ±: Sayfa iÃ§eriÄŸi Ã§ekilemedi.")
            return 0
        except Exception as e:
            self._send_feedback(f"Wikipedia BaÄŸlantÄ±/API HatasÄ±: {e}")
            return 0
        
        return total_learned


    def get_response(self, user_input):
        
        user_input_lower = user_input.lower()
        
        # 1. AÅŸama: BulanÄ±k EÅŸleÅŸtirme ile HafÄ±zayÄ± Kontrol Et
        
        # KullanÄ±cÄ± girdisini temizle (yazÄ±m/noktalama hatalarÄ±nÄ± ignore etmek iÃ§in)
        cleaned_input = self._clean_text(user_input_lower)
        
        # HafÄ±zadaki tÃ¼m sorularÄ± al
        all_questions = [entry['soru'] for entry in self.kb.get_all_knowledge()]
        
        if all_questions:
            # BulanÄ±k eÅŸleÅŸtirme yap: TemizlenmiÅŸ girdiye en Ã§ok benzeyen soruyu bul
            # Score eÅŸiÄŸini 85 olarak belirle (%85 benzerlik yeterli)
            match_result = process.extractOne(cleaned_input, all_questions, scorer=fuzz.ratio)
            
            best_match_question = match_result[0] # En iyi eÅŸleÅŸen soru
            best_score = match_result[1]         # Benzerlik puanÄ±

            if best_score >= 85: # EÅŸik deÄŸeri: %85 ve Ã¼zeri kabul
                
                # EÅŸleÅŸen soruyu kullanarak bilgiyi KnowledgeBase'ten bul
                for entry in self.kb.get_all_knowledge():
                    if entry['soru'] == best_match_question:
                        self._send_feedback(f"ğŸ§  HafÄ±za eÅŸleÅŸti: '{best_match_question}' (%{best_score} benzerlik)")
                        return f"ğŸ¤– Cevap: {entry['cevap']} (Kaynak: {entry['kaynak']})"
        
        # EÄŸer sohbet kalÄ±plarÄ±ndan bir cevap bulunamazsa, Wikipedia arama ve Ã¶ÄŸrenmeye devam et
        self._send_feedback("ğŸ” HafÄ±zada net eÅŸleÅŸme bulunamadÄ±. Wikipedia araÅŸtÄ±rmasÄ± baÅŸlÄ±yor...")
        
        # 2. AÅŸama: Wikipedia'dan araÅŸtÄ±r ve Ã¶ÄŸren (Kalan kod aynÄ±)
        try:
            wikipedia.set_lang("tr") 
            
            # Wikipedia, arama terimini otomatik olarak dÃ¼zeltir ve en iyi baÅŸlÄ±ÄŸÄ± dÃ¶ndÃ¼rÃ¼r
            corrected_query = wikipedia.search(user_input, results=1)
            
            if corrected_query:
                # DÃ¼zeltilmiÅŸ baÅŸlÄ±ÄŸÄ±/sorguyu kullan
                search_query = corrected_query[0]
                self._send_feedback(f"ğŸ’¡ Sorgu DÃ¼zeltildi: '{user_input}' yerine '{search_query}' aranacak.")
            else:
                search_query = user_input # BaÅŸlÄ±k bulunamazsa orijinalini kullan
                
        except Exception:
            search_query = user_input # Hata olursa orijinalini 
            
        learned_count = self.search_and_learn_online(search_query)
        
        if learned_count > 0:
            # Yeni Ã¶ÄŸrenilen bilgiyi kullanarak tekrar cevap ara
            for entry in self.kb.get_all_knowledge():
                if user_input.lower() in entry['soru'].lower() or \
                   user_input.lower() in entry['cevap'].lower():
                    return f"ğŸ’¡ Yeni Ã¶ÄŸrendiÄŸim bilgiye gÃ¶re: {entry['cevap']} "
            else:
                 return f"ğŸ’¡ Yeni bir ÅŸeyler Ã¶ÄŸrendim ({learned_count} yeni bilgi), ancak sorunuzun spesifik cevabÄ±nÄ± henÃ¼z Ã§Ä±karamadÄ±m."
        else:
            # Ã–ÄŸrenme baÅŸarÄ±sÄ±z oldu
            return "ğŸ˜” ÃœzgÃ¼nÃ¼m, bu konuyu henÃ¼z bilmiyorum ve Wikipedia'da da anlamlÄ± bir bilgi bulamadÄ±m."
        
    def generate_self_query(self):
        """
        HafÄ±zadaki mevcut bilgileri kullanarak yeni bir arama sorgusu Ã¼retir.
        """
        all_knowledge = self.kb.get_all_knowledge()
        if not all_knowledge:
            return "Genel kÃ¼ltÃ¼r nedir?" # BaÅŸlangÄ±Ã§ sorgusu

        # HafÄ±zadaki rastgele bir bilgiyi Ã§ek
        import random
        random_entry = random.choice(all_knowledge)

        # Yeni sorguyu, mevcut bilginin kaynaÄŸÄ±na veya cevabÄ±na dayandÄ±r
        query_type = random.choice(['kaynak', 'cevap'])
        
        if query_type == 'kaynak' and 'Wikipedia' in random_entry['kaynak']:
            # Wikipedia'nÄ±n Ã¶ÄŸrendiÄŸi sayfa adÄ±nÄ± alÄ±p, 'nedir?' ekle
            base_query = random_entry['kaynak'].replace("Wikipedia: ", "").strip()
            self._send_feedback(f"ğŸ§  Ä°Ã§ sorgu: '{base_query}' bilgisinin derinleÅŸtirilmesi...")
            return base_query + " tarihÃ§esi" # Konuyu derinleÅŸtir

        elif query_type == 'cevap':
            # CevabÄ±n ilk 4 kelimesini alÄ±p "ne iÅŸe yarar?" diye sor
            tokens = word_tokenize(random_entry['cevap'])
            if len(tokens) > 5:
                base_query = ' '.join(tokens[:4])
                self._send_feedback(f"ğŸ§  Ä°Ã§ sorgu: '{base_query}' hakkÄ±nda ek bilgi aranÄ±yor...")
                return base_query + " faydalarÄ±"
                
        # HiÃ§biri tutmazsa, rastgele bir popÃ¼ler konuya dÃ¶n
        return random.choice(["KÃ¼resel Ä±sÄ±nma nedir?", "Kara delikler nasÄ±l oluÅŸur?", "Ä°nsan beyni hakkÄ±nda bilgi"])
    
    
    def self_learn_cycle(self):
        """
        AsistanÄ±n kendi kendine bir sorgu Ã¼rettiÄŸi ve Ã¶ÄŸrendiÄŸi ana dÃ¶ngÃ¼.
        """
        query = self.generate_self_query()
        
        self._send_feedback("--------------------------------------------------")
        self._send_feedback(f"ğŸ¤– KENDÄ° KENDÄ°NE Ã–ÄRENME BAÅLATILDI. Yeni sorgu: {query}")
        
        initial_count = self.kb.get_knowledge_count()
        
        # Sorguyu Wikipedia'ya gÃ¶nder
        learned_count = self.search_and_learn_online(query)
        
        final_count = self.kb.get_knowledge_count()

        if learned_count > 0:
            self._send_feedback(f"âœ… Ã–ÄŸrenme tamamlandÄ±. {learned_count} yeni bilgi eklendi. Toplam bilgi: {final_count}")
        else:
            self._send_feedback("âŒ Kendi kendine Ã¶ÄŸrenme baÅŸarÄ±lÄ± olamadÄ± veya yeni bilgi bulunamadÄ±.")
        self._send_feedback("--------------------------------------------------")
        
        return learned_count